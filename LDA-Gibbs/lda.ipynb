{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) with Gibbs Sampling for Topic Modeling\n",
    "Following the guidelines of the technical report [*A Theoretical and Practical Implementation\n",
    "Tutorial on Topic Modeling and Gibbs Sampling*](https://coli-saar.github.io/cl19/materials/darling-lda.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/emails.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirichlet, hyperparams, tokenizer\n",
    "ALPHA = 0.1\n",
    "BETA = 0.1\n",
    "NUM_TOPICS = 20\n",
    "\n",
    "sp = spacy.load('en_core_web_sm') # tokenizer\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(data, max_docs=10000):\n",
    "    freqs = Counter()\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    all_stopwords.add('enron')\n",
    "    nr_tokens = 0\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "        for token in tokens:\n",
    "            if token.text.lower() not in all_stopwords and token.is_alpha:\n",
    "                freqs[token.text.lower()] += 1\n",
    "                nr_tokens += 1\n",
    "    \n",
    "    return freqs\n",
    "\n",
    "def get_vocab(freqs, freq_threshold=3):\n",
    "    vocab = {}\n",
    "    vocab_idx_str = {}\n",
    "    vocab_idx = 0\n",
    "\n",
    "    for word in freqs:\n",
    "        if freqs[word] >= freq_threshold:\n",
    "            vocab[word] = vocab_idx\n",
    "            vocab_idx_str[vocab_idx] = word\n",
    "            vocab_idx += 1\n",
    "\n",
    "    return vocab, vocab_idx_str\n",
    "\n",
    "def tokenize_dataset(data, vocab, max_docs=10000):\n",
    "    nr_tokens = 0\n",
    "    nr_docs = 0\n",
    "    docs = []\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            doc = []\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text in vocab:\n",
    "                    doc.append(token_text)\n",
    "                    nr_tokens += 1\n",
    "            nr_docs += 1\n",
    "            docs.append(doc)\n",
    "        \n",
    "    print(f\"Number of emails: {nr_docs}, Number of tokens: {nr_tokens}\")\n",
    "\n",
    "    # Numericalize the dataset\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        corpus_d = []\n",
    "        \n",
    "        for token in doc:\n",
    "            corpus_d.append(vocab[token])\n",
    "        \n",
    "        corpus.append(np.asarray(corpus_d))\n",
    "\n",
    "    return docs, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails: 10000, Number of tokens: 1992931\n",
      "Vocab size: 27877\n"
     ]
    }
   ],
   "source": [
    "data = df['message'].sample(frac=1, random_state=42).values\n",
    "freqs = generate_frequencies(data, max_docs=10000)\n",
    "vocab, vocab_idx_str = get_vocab(freqs)\n",
    "docs, corpus = tokenize_dataset(data, vocab)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [31:25<00:00,  9.43s/it]\n"
     ]
    }
   ],
   "source": [
    "def LDA_Collapsed_Gibbs(corpus, num_iter=200):\n",
    "    # Initialize the counts and Z\n",
    "    Z = []\n",
    "    num_docs = len(corpus)\n",
    "    for _, doc in enumerate(corpus):\n",
    "        Z_d = np.random.randint(low=0, high=NUM_TOPICS, size=len(doc))\n",
    "        Z.append(Z_d)\n",
    "    \n",
    "    ndk = np.zeros((num_docs, NUM_TOPICS))\n",
    "    for d in range(num_docs):\n",
    "        for k in range(NUM_TOPICS):\n",
    "            ndk[d, k] = np.sum(Z[d] == k)\n",
    "    \n",
    "    nkw = np.zeros((NUM_TOPICS, vocab_size))\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        for word_idx, word in enumerate(doc):\n",
    "            topic = Z[doc_idx][word_idx]\n",
    "            nkw[topic, word] += 1\n",
    "    \n",
    "    nk = np.sum(nkw, axis=1)\n",
    "    topic_list = [i for i in range(NUM_TOPICS)]\n",
    "\n",
    "    # Loop\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            for i in range(len(doc)):\n",
    "                word = doc[i]\n",
    "                topic = Z[doc_idx][i]\n",
    "\n",
    "                # Remove Z_i because conditioned on Z_(-i)\n",
    "                ndk[doc_idx, topic] -= 1\n",
    "                nkw[topic, word] -= 1\n",
    "                nk[topic] -= 1\n",
    "\n",
    "                p_z = (ndk[doc_idx, :] + ALPHA) * (nkw[:, word] + BETA) / (nk + BETA * vocab_size)\n",
    "                topic = random.choices(topic_list, weights=p_z, k=1)[0]\n",
    "\n",
    "                # Update n parameters\n",
    "                Z[doc_idx][i] = topic\n",
    "                ndk[doc_idx, topic] += 1\n",
    "                nkw[topic, word] += 1\n",
    "                nk[topic] += 1\n",
    "    \n",
    "    return Z, ndk, nkw, nk\n",
    "\n",
    "Z, ndk, nkw, nk = LDA_Collapsed_Gibbs(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 most common words: \n",
      "x\n",
      "subject\n",
      "cc\n",
      "content\n",
      "kaminski\n",
      "vince\n",
      "j\n",
      "pm\n",
      "meeting\n",
      "bcc\n",
      "\n",
      "\n",
      "Topic 1 most common words: \n",
      "power\n",
      "energy\n",
      "state\n",
      "california\n",
      "said\n",
      "electricity\n",
      "davis\n",
      "gas\n",
      "utilities\n",
      "prices\n",
      "\n",
      "\n",
      "Topic 2 most common words: \n",
      "x\n",
      "hou\n",
      "cc\n",
      "subject\n",
      "jones\n",
      "sara\n",
      "shackleton\n",
      "mark\n",
      "tana\n",
      "legal\n",
      "\n",
      "\n",
      "Topic 3 most common words: \n",
      "x\n",
      "subject\n",
      "cc\n",
      "tw\n",
      "fossum\n",
      "content\n",
      "scott\n",
      "drew\n",
      "blair\n",
      "sent\n",
      "\n",
      "\n",
      "Topic 4 most common words: \n",
      "pm\n",
      "scheduled\n",
      "sat\n",
      "x\n",
      "access\n",
      "outages\n",
      "time\n",
      "system\n",
      "london\n",
      "fri\n",
      "\n",
      "\n",
      "Topic 5 most common words: \n",
      "gas\n",
      "deal\n",
      "x\n",
      "price\n",
      "subject\n",
      "day\n",
      "market\n",
      "deals\n",
      "cc\n",
      "month\n",
      "\n",
      "\n",
      "Topic 6 most common words: \n",
      "x\n",
      "jeff\n",
      "dasovich\n",
      "na\n",
      "kean\n",
      "j\n",
      "richard\n",
      "james\n",
      "symes\n",
      "steffes\n",
      "\n",
      "\n",
      "Topic 7 most common words: \n",
      "cn\n",
      "recipients\n",
      "ou\n",
      "na\n",
      "notesaddr\n",
      "non\n",
      "john\n",
      "x\n",
      "eu\n",
      "david\n",
      "\n",
      "\n",
      "Topic 8 most common words: \n",
      "image\n",
      "x\n",
      "click\n",
      "free\n",
      "email\n",
      "content\n",
      "e\n",
      "new\n",
      "web\n",
      "information\n",
      "\n",
      "\n",
      "Topic 9 most common words: \n",
      "need\n",
      "issues\n",
      "time\n",
      "market\n",
      "information\n",
      "business\n",
      "work\n",
      "process\n",
      "group\n",
      "risk\n",
      "\n",
      "\n",
      "Topic 10 most common words: \n",
      "company\n",
      "said\n",
      "stock\n",
      "new\n",
      "billion\n",
      "million\n",
      "companies\n",
      "financial\n",
      "dow\n",
      "year\n",
      "\n",
      "\n",
      "Topic 11 most common words: \n",
      "kay\n",
      "mann\n",
      "agreement\n",
      "error\n",
      "x\n",
      "subject\n",
      "cc\n",
      "database\n",
      "pm\n",
      "corp\n",
      "\n",
      "\n",
      "Topic 12 most common words: \n",
      "hou\n",
      "corp\n",
      "john\n",
      "x\n",
      "david\n",
      "na\n",
      "mark\n",
      "l\n",
      "ect\n",
      "j\n",
      "\n",
      "\n",
      "Topic 13 most common words: \n",
      "energy\n",
      "new\n",
      "business\n",
      "company\n",
      "services\n",
      "president\n",
      "management\n",
      "global\n",
      "industry\n",
      "power\n",
      "\n",
      "\n",
      "Topic 14 most common words: \n",
      "know\n",
      "going\n",
      "good\n",
      "like\n",
      "think\n",
      "subject\n",
      "time\n",
      "day\n",
      "night\n",
      "people\n",
      "\n",
      "\n",
      "Topic 15 most common words: \n",
      "x\n",
      "content\n",
      "cc\n",
      "bcc\n",
      "subject\n",
      "message\n",
      "version\n",
      "date\n",
      "folder\n",
      "id\n",
      "\n",
      "\n",
      "Topic 16 most common words: \n",
      "x\n",
      "message\n",
      "mail\n",
      "e\n",
      "subject\n",
      "intended\n",
      "sent\n",
      "cc\n",
      "information\n",
      "content\n",
      "\n",
      "\n",
      "Topic 17 most common words: \n",
      "travel\n",
      "houston\n",
      "tx\n",
      "hotel\n",
      "new\n",
      "city\n",
      "day\n",
      "court\n",
      "way\n",
      "san\n",
      "\n",
      "\n",
      "Topic 18 most common words: \n",
      "e\n",
      "mail\n",
      "updated\n",
      "game\n",
      "week\n",
      "class\n",
      "wr\n",
      "texas\n",
      "align\n",
      "rb\n",
      "\n",
      "\n",
      "Topic 19 most common words: \n",
      "x\n",
      "content\n",
      "cc\n",
      "subject\n",
      "date\n",
      "bcc\n",
      "type\n",
      "plain\n",
      "id\n",
      "version\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phi = nkw / nk.reshape(NUM_TOPICS, 1) # to get the probability distribution\n",
    "\n",
    "num_words = 10\n",
    "\n",
    "for k in range(NUM_TOPICS):\n",
    "    most_common_words = np.argsort(phi[k])[::-1][:num_words]\n",
    "    print(f\"Topic {k} most common words: \")\n",
    "    \n",
    "    for word_idx in most_common_words:\n",
    "        print(vocab_idx_str[word_idx])\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
